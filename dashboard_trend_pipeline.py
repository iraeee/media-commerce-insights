"""
dashboard_trend_pipeline.py - ì¶”ì„¸ë¶„ì„ ë°ì´í„° íŒŒì´í”„ë¼ì¸
Version: 1.1.0
Created: 2025-01-25
Updated: 2025-09-12 - ë°ì´í„° íƒ€ì… ì•ˆì •ì„± ê°•í™”

ë°ì´í„° íë¦„: Raw Data â†’ ì „ì²˜ë¦¬ â†’ ì§‘ê³„ â†’ ì§€í‘œê³„ì‚° â†’ ìºì‹± â†’ ì‹œê°í™”
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import hashlib
import json
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# ì¶”ì„¸ ê³„ì‚°ê¸° import
from dashboard_trend_calculator import TrendCalculator

class TrendDataPipeline:
    """ì¶”ì„¸ë¶„ì„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, db_path="schedule.db", cache_dir="cache"):
        """
        ì´ˆê¸°í™”
        
        Parameters:
        -----------
        db_path : str
            ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        cache_dir : str
            ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.db_path = db_path
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_ttl = 3600  # 1ì‹œê°„ ìºì‹œ
        self.calculator = TrendCalculator()
        
        # ìƒë°©ì†¡ ì±„ë„ ì •ì˜ (dashboard_configì™€ ë™ì¼)
        self.LIVE_CHANNELS = {
            'í˜„ëŒ€í™ˆì‡¼í•‘', 'GSí™ˆì‡¼í•‘', 'ë¡¯ë°í™ˆì‡¼í•‘', 'CJì˜¨ìŠ¤íƒ€ì¼', 
            'í™ˆì•¤ì‡¼í•‘', 'NSí™ˆì‡¼í•‘', 'ê³µì˜ì‡¼í•‘'
        }
        
        # ëª¨ë¸ë¹„ ì„¤ì •
        self.MODEL_COST_LIVE = 10400000
        self.MODEL_COST_NON_LIVE = 2000000
        
        # ROI ê³„ì‚° ì„¤ì •
        self.CONVERSION_RATE = 0.75
        self.PRODUCT_COST_RATE = 0.13
        self.COMMISSION_RATE = 0.10
        self.REAL_MARGIN_RATE = (1 - self.COMMISSION_RATE - self.PRODUCT_COST_RATE) * self.CONVERSION_RATE
    
    def execute_pipeline(self, 
                        date_range: Tuple[str, str] = None,
                        filters: Dict[str, Any] = None,
                        use_cache: bool = True,
                        source: str = 'schedule') -> Dict[str, pd.DataFrame]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Parameters:
        -----------
        date_range : tuple
            (ì‹œì‘ì¼, ì¢…ë£Œì¼) í˜•ì‹
        filters : dict
            í•„í„° ì¡°ê±´ (platforms, categories ë“±)
        use_cache : bool
            ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        source : str
            ë°ì´í„° ì†ŒìŠ¤ ('schedule' or 'broadcasts')
            
        Returns:
        --------
        dict : ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_cache_key(date_range, filters)
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached_data = self._load_from_cache(cache_key)
            if cached_data is not None:
                print("ğŸ“¦ ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ")
                return cached_data
        
        print("ğŸ”„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
        
        # Step 1: ë°ì´í„° ì¶”ì¶œ (íƒ€ì… ì•ˆì •ì„± ê°•í™”)
        raw_data = self.extract_data(date_range, filters, source)
        
        if raw_data.empty:
            print("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {'daily': pd.DataFrame(), 'category': pd.DataFrame()}
        
        # Step 2: ë°ì´í„° ì •ì œ (íƒ€ì… ê²€ì¦ ê°•í™”)
        cleaned_data = self.clean_data(raw_data)
        
        # Step 3: ì‹œê³„ì—´ ì§‘ê³„
        aggregated_data = self.aggregate_timeseries(cleaned_data)
        
        # Step 4: ì¶”ì„¸ ì§€í‘œ ê³„ì‚°
        trend_metrics = self.calculate_trends(aggregated_data)
        
        # Step 5: ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        category_trends = self.analyze_category_trends(cleaned_data)
        
        # Step 6: ê²°ê³¼ êµ¬ì„±
        result = {
            'daily': trend_metrics['daily'],
            'weekly': trend_metrics.get('weekly', pd.DataFrame()),
            'monthly': trend_metrics.get('monthly', pd.DataFrame()),
            'category': category_trends,
            'raw': cleaned_data,
            'summary': self._create_summary(trend_metrics)
        }
        
        # Step 7: ìºì‹±
        if use_cache:
            self._save_to_cache(cache_key, result)
        
        print("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
        return result
    
    def extract_data(self, date_range: Tuple[str, str] = None, 
                    filters: Dict[str, Any] = None,
                    source: str = 'schedule') -> pd.DataFrame:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ (íƒ€ì… ì•ˆì •ì„± ê°•í™”)
        
        Parameters:
        -----------
        date_range : tuple
            (ì‹œì‘ì¼, ì¢…ë£Œì¼)
        filters : dict
            í•„í„° ì¡°ê±´
        source : str
            ë°ì´í„° ì†ŒìŠ¤ ('schedule' or 'broadcasts')
            
        Returns:
        --------
        DataFrame : ì¶”ì¶œëœ ë°ì´í„°
        """
        conn = sqlite3.connect(self.db_path)
        
        # ë°ì´í„° ì†ŒìŠ¤ì— ë”°ë¥¸ ì¿¼ë¦¬ ì„ íƒ
        if source == 'schedule':
            # schedule í…Œì´ë¸” ì‚¬ìš© (ë°ì´í„° ì™„ì „ì„±ì´ ë” ë†’ìŒ)
            query = """
                SELECT 
                    date, 
                    time, 
                    broadcast, 
                    platform, 
                    category,
                    CAST(revenue AS REAL) as revenue,
                    CAST(COALESCE(cost, 0) AS REAL) as cost,
                    CAST(COALESCE(roi_calculated, 0) AS REAL) as roi,
                    CAST(units_sold AS INTEGER) as units_sold,
                    CAST(product_count AS INTEGER) as product_count
                FROM schedule 
                WHERE platform != 'ê¸°íƒ€'
            """
        else:
            # broadcasts í…Œì´ë¸” ì‚¬ìš© ì‹œ NULL ì œì™¸ ë° íƒ€ì… ìºìŠ¤íŒ…
            query = """
                SELECT 
                    date, 
                    time, 
                    broadcast_name as broadcast, 
                    platform, 
                    category,
                    CAST(revenue AS REAL) as revenue,
                    CAST(COALESCE(cost, 0) AS REAL) as cost,
                    CAST(COALESCE(roi, 0) AS REAL) as roi,
                    CAST(units_sold AS INTEGER) as units_sold,
                    CAST(product_count AS INTEGER) as product_count
                FROM broadcasts
                WHERE platform != 'ê¸°íƒ€'
                  AND revenue IS NOT NULL
            """
        
        conditions = []
        params = []
        
        # ë‚ ì§œ í•„í„°
        if date_range:
            start_date, end_date = date_range
            conditions.append("date BETWEEN ? AND ?")
            params.extend([start_date, end_date])
        
        # í”Œë«í¼ í•„í„°
        if filters and 'platforms' in filters and filters['platforms']:
            placeholders = ','.join(['?' for _ in filters['platforms']])
            conditions.append(f"platform IN ({placeholders})")
            params.extend(filters['platforms'])
        
        # ì¹´í…Œê³ ë¦¬ í•„í„°
        if filters and 'categories' in filters and filters['categories']:
            placeholders = ','.join(['?' for _ in filters['categories']])
            conditions.append(f"category IN ({placeholders})")
            params.extend(filters['categories'])
        
        # ë§¤ì¶œ ìƒí•œ í•„í„°
        if filters and 'revenue_limit' in filters:
            conditions.append("revenue <= ?")
            params.append(filters['revenue_limit'])
        
        # ì¡°ê±´ ì¶”ê°€
        if conditions:
            query += " AND " + " AND ".join(conditions)
        
        query += " ORDER BY date, time"
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()
        
        # ì¶”ê°€ íƒ€ì… ë³€í™˜ ë³´ì¥
        numeric_columns = ['revenue', 'cost', 'roi', 'units_sold', 'product_count']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        print(f"ğŸ“Š {len(df):,}ê°œ ë ˆì½”ë“œ ì¶”ì¶œ (ì†ŒìŠ¤: {source})")
        return df
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ë°ì´í„° ì •ì œ ë° ì „ì²˜ë¦¬ (íƒ€ì… ê²€ì¦ ê°•í™”)
        
        Parameters:
        -----------
        df : DataFrame
            ì›ë³¸ ë°ì´í„°
            
        Returns:
        --------
        DataFrame : ì •ì œëœ ë°ì´í„°
        """
        if df.empty:
            return df
        
        df = df.copy()
        
        # ë‚ ì§œ ë³€í™˜
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        
        # ìˆ«ì ì»¬ëŸ¼ ê°•ì œ ë³€í™˜ ë° ê²€ì¦
        numeric_columns = ['revenue', 'cost', 'roi', 'units_sold', 'product_count']
        
        for col in numeric_columns:
            if col in df.columns:
                # ë³€í™˜ ì „ ë°ì´í„° íƒ€ì… ë¡œê¹…
                original_dtype = df[col].dtype
                
                # ìˆ«ìë¡œ ë³€í™˜
                df[col] = pd.to_numeric(df[col], errors='coerce')
                
                # NULL ê°’ ì²˜ë¦¬
                if col in ['cost', 'roi']:
                    # costì™€ roiëŠ” 0ìœ¼ë¡œ ì±„ìš°ê¸°
                    df[col] = df[col].fillna(0)
                else:
                    # ë‹¤ë¥¸ ì»¬ëŸ¼ì€ ì „ë°© ì±„ìš°ê¸° í›„ 0ìœ¼ë¡œ ì±„ìš°ê¸°
                    df[col] = df[col].fillna(method='ffill').fillna(0)
                
                # ë³€í™˜ ê²°ê³¼ ê²€ì¦
                if df[col].dtype not in ['float64', 'int64']:
                    print(f"âš ï¸ {col} ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ê²½ê³ : {original_dtype} â†’ {df[col].dtype}")
        
        # ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ ìƒì„±
        df['hour'] = df['time'].str.split(':').str[0].astype(int)
        df['weekday'] = df['date'].dt.dayofweek
        df['weekday_name'] = df['date'].dt.day_name()
        df['month'] = df['date'].dt.month
        df['quarter'] = df['date'].dt.quarter
        df['year'] = df['date'].dt.year
        df['week'] = df['date'].dt.strftime('%Y-W%U')
        df['year_month'] = df['date'].dt.strftime('%Y-%m')
        df['is_weekend'] = df['weekday'].isin([5, 6])
        
        # ì±„ë„ êµ¬ë¶„
        df['is_live'] = df['platform'].isin(self.LIVE_CHANNELS).astype(int)
        df['channel_type'] = df['is_live'].map({1: 'ìƒë°©ì†¡', 0: 'ë¹„ìƒë°©ì†¡'})
        
        # ë¹„ìš© ê³„ì‚°
        df['model_cost'] = np.where(df['is_live'], 
                                    self.MODEL_COST_LIVE, 
                                    self.MODEL_COST_NON_LIVE)
        df['total_cost'] = df['cost'] + df['model_cost']
        
        # ì‹¤ì§ˆ ìˆ˜ìµ ê³„ì‚° (ìƒˆë¡œìš´ ROI ê³„ì‚°ë²• ì ìš©)
        df['real_profit'] = (df['revenue'] * self.REAL_MARGIN_RATE) - df['total_cost']
        
        # ROI ê³„ì‚°
        df['roi_calculated'] = np.where(
            df['total_cost'] > 0,
            (df['real_profit'] / df['total_cost']) * 100,
            0
        )
        
        # íš¨ìœ¨ì„± ì§€í‘œ
        df['efficiency'] = np.where(
            df['total_cost'] > 0,
            df['revenue'] / df['total_cost'],
            0
        )
        
        # ë‹¨ê°€ ê³„ì‚°
        df['unit_price'] = np.where(
            df['units_sold'] > 0,
            df['revenue'] / df['units_sold'],
            0
        )
        
        # ì´ìƒì¹˜ ì œê±° (ìŒìˆ˜ ë§¤ì¶œ)
        df = df[df['revenue'] >= 0]
        
        print(f"âœ¨ ë°ì´í„° ì •ì œ ì™„ë£Œ ({len(df)}ê°œ ë ˆì½”ë“œ)")
        return df
    
    def aggregate_timeseries(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """
        ì‹œê³„ì—´ ì§‘ê³„
        
        Parameters:
        -----------
        df : DataFrame
            ì •ì œëœ ë°ì´í„°
            
        Returns:
        --------
        dict : ê¸°ê°„ë³„ ì§‘ê³„ ë°ì´í„°
        """
        aggregations = {}
        
        # ì¼ë³„ ì§‘ê³„
        daily = df.groupby('date').agg({
            'revenue': 'sum',
            'units_sold': 'sum',
            'roi_calculated': 'mean',
            'total_cost': 'sum',
            'real_profit': 'sum',
            'broadcast': 'count',
            'efficiency': 'mean'
        }).reset_index()
        
        daily.columns = ['date', 'revenue', 'units_sold', 'avg_roi', 
                         'total_cost', 'real_profit', 'broadcast_count', 'avg_efficiency']
        aggregations['daily'] = daily
        
        # ì£¼ë³„ ì§‘ê³„
        weekly = df.groupby('week').agg({
            'date': ['min', 'max'],
            'revenue': 'sum',
            'units_sold': 'sum',
            'roi_calculated': 'mean',
            'total_cost': 'sum',
            'real_profit': 'sum',
            'broadcast': 'count'
        }).reset_index()
        
        weekly.columns = ['week', 'start_date', 'end_date', 'revenue', 
                         'units_sold', 'avg_roi', 'total_cost', 'real_profit', 'broadcast_count']
        aggregations['weekly'] = weekly
        
        # ì›”ë³„ ì§‘ê³„
        monthly = df.groupby('year_month').agg({
            'revenue': 'sum',
            'units_sold': 'sum',
            'roi_calculated': 'mean',
            'total_cost': 'sum',
            'real_profit': 'sum',
            'broadcast': 'count'
        }).reset_index()
        
        monthly.columns = ['year_month', 'revenue', 'units_sold', 'avg_roi', 
                          'total_cost', 'real_profit', 'broadcast_count']
        aggregations['monthly'] = monthly
        
        print(f"ğŸ“ˆ ì‹œê³„ì—´ ì§‘ê³„ ì™„ë£Œ (ì¼ë³„: {len(daily)}, ì£¼ë³„: {len(weekly)}, ì›”ë³„: {len(monthly)})")
        return aggregations
    
    def calculate_trends(self, aggregated_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """
        ì¶”ì„¸ ì§€í‘œ ê³„ì‚°
        
        Parameters:
        -----------
        aggregated_data : dict
            ì§‘ê³„ëœ ë°ì´í„°
            
        Returns:
        --------
        dict : ì¶”ì„¸ ì§€í‘œê°€ ì¶”ê°€ëœ ë°ì´í„°
        """
        result = {}
        
        # ì¼ë³„ ì¶”ì„¸
        if 'daily' in aggregated_data and not aggregated_data['daily'].empty:
            daily = aggregated_data['daily'].copy()
            
            # ëª¨ë“  ì¶”ì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°
            daily = self.calculator.calculate_growth_rates(daily)
            daily = self.calculator.calculate_moving_averages(daily)
            daily = self.calculator.calculate_volatility(daily)
            daily = self.calculator.detect_trend_direction(daily)
            daily = self.calculator.calculate_seasonality(daily)
            daily = self.calculator.detect_anomalies(daily)
            
            result['daily'] = daily
        
        # ì£¼ë³„ ì¶”ì„¸
        if 'weekly' in aggregated_data and not aggregated_data['weekly'].empty:
            weekly = aggregated_data['weekly'].copy()
            weekly['revenue_wow'] = weekly['revenue'].pct_change() * 100
            weekly['revenue_4w'] = weekly['revenue'].pct_change(periods=4) * 100
            weekly['ma_4w'] = weekly['revenue'].rolling(window=4, min_periods=1).mean()
            result['weekly'] = weekly
        
        # ì›”ë³„ ì¶”ì„¸
        if 'monthly' in aggregated_data and not aggregated_data['monthly'].empty:
            monthly = aggregated_data['monthly'].copy()
            monthly['revenue_mom'] = monthly['revenue'].pct_change() * 100
            monthly['revenue_yoy'] = monthly['revenue'].pct_change(periods=12) * 100
            
            # ê³„ì ˆ ì§€ìˆ˜
            monthly_avg = monthly['revenue'].mean()
            monthly['seasonal_index'] = (monthly['revenue'] / monthly_avg) * 100
            
            result['monthly'] = monthly
        
        print("ğŸ“Š ì¶”ì„¸ ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
        return result
    
    def analyze_category_trends(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì„¸ ë¶„ì„
        
        Parameters:
        -----------
        df : DataFrame
            ì •ì œëœ ë°ì´í„°
            
        Returns:
        --------
        DataFrame : ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì„¸ ë°ì´í„°
        """
        if df.empty:
            return pd.DataFrame()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¼ë³„ ì§‘ê³„
        category_daily = df.groupby(['date', 'category']).agg({
            'revenue': 'sum',
            'units_sold': 'sum',
            'roi_calculated': 'mean',
            'broadcast': 'count'
        }).reset_index()
        
        # ì¼ë³„ ì´ ë§¤ì¶œë¡œ ì‹œì¥ ì ìœ ìœ¨ ê³„ì‚°
        daily_total = category_daily.groupby('date')['revenue'].sum().reset_index()
        daily_total.columns = ['date', 'total_revenue']
        
        category_daily = category_daily.merge(daily_total, on='date')
        category_daily['market_share'] = (category_daily['revenue'] / 
                                          category_daily['total_revenue']) * 100
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ì¥ë¥ 
        category_daily['growth_rate'] = category_daily.groupby('category')['revenue'].pct_change() * 100
        
        # ì£¼ë³„ ì§‘ê³„ë¡œ ë³€í™˜ (íˆíŠ¸ë§µìš©)
        category_daily['week'] = pd.to_datetime(category_daily['date']).dt.strftime('%Y-W%U')
        
        category_weekly = category_daily.groupby(['week', 'category']).agg({
            'revenue': 'sum',
            'market_share': 'mean',
            'growth_rate': 'mean',
            'broadcast': 'sum'
        }).reset_index()
        
        # ì¶”ì„¸ ë°©í–¥ ê³„ì‚°
        def calculate_trend(group):
            if len(group) < 3:
                return 'stable'
            recent_avg = group['revenue'].tail(3).mean()
            prev_avg = group['revenue'].head(3).mean()
            if recent_avg > prev_avg * 1.1:
                return 'up'
            elif recent_avg < prev_avg * 0.9:
                return 'down'
            return 'stable'
        
        category_trends = category_daily.groupby('category').apply(calculate_trend).reset_index()
        category_trends.columns = ['category', 'trend_direction']
        
        # ëª¨ë©˜í…€ ìŠ¤ì½”ì–´
        momentum = category_daily.groupby('category')['growth_rate'].mean().reset_index()
        momentum.columns = ['category', 'momentum_score']
        
        # ë³‘í•©
        category_weekly = category_weekly.merge(category_trends, on='category', how='left')
        category_weekly = category_weekly.merge(momentum, on='category', how='left')
        
        print(f"ğŸ“¦ ì¹´í…Œê³ ë¦¬ ì¶”ì„¸ ë¶„ì„ ì™„ë£Œ ({len(category_weekly)}ê°œ ë ˆì½”ë“œ)")
        return category_weekly
    
    def _generate_cache_key(self, date_range: Tuple[str, str] = None, 
                           filters: Dict[str, Any] = None) -> str:
        """
        ìºì‹œ í‚¤ ìƒì„±
        
        Parameters:
        -----------
        date_range : tuple
            ë‚ ì§œ ë²”ìœ„
        filters : dict
            í•„í„° ì¡°ê±´
            
        Returns:
        --------
        str : ìºì‹œ í‚¤
        """
        key_data = {
            'date_range': date_range,
            'filters': filters,
            'version': '1.1.0'
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[Dict]:
        """
        ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ
        
        Parameters:
        -----------
        cache_key : str
            ìºì‹œ í‚¤
            
        Returns:
        --------
        dict or None : ìºì‹œëœ ë°ì´í„°
        """
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if not cache_file.exists():
            return None
        
        # ìºì‹œ ë§Œë£Œ í™•ì¸
        file_age = datetime.now().timestamp() - cache_file.stat().st_mtime
        if file_age > self.cache_ttl:
            return None
        
        try:
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _save_to_cache(self, cache_key: str, data: Dict):
        """
        ìºì‹œì— ë°ì´í„° ì €ì¥
        
        Parameters:
        -----------
        cache_key : str
            ìºì‹œ í‚¤
        data : dict
            ì €ì¥í•  ë°ì´í„°
        """
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
            print(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _create_summary(self, trend_metrics: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """
        ì¢…í•© ìš”ì•½ ìƒì„±
        
        Parameters:
        -----------
        trend_metrics : dict
            ì¶”ì„¸ ë©”íŠ¸ë¦­ ë°ì´í„°
            
        Returns:
        --------
        dict : ìš”ì•½ í†µê³„
        """
        summary = {}
        
        if 'daily' in trend_metrics and not trend_metrics['daily'].empty:
            daily = trend_metrics['daily']
            
            summary['ê¸°ê°„'] = f"{daily['date'].min()} ~ {daily['date'].max()}"
            summary['ì´_ì¼ìˆ˜'] = len(daily)
            summary['ì´_ë§¤ì¶œ'] = daily['revenue'].sum()
            summary['í‰ê· _ì¼ë§¤ì¶œ'] = daily['revenue'].mean()
            summary['ìµœëŒ€_ì¼ë§¤ì¶œ'] = daily['revenue'].max()
            summary['ìµœì†Œ_ì¼ë§¤ì¶œ'] = daily['revenue'].min()
            
            if 'revenue_dod' in daily.columns:
                summary['í‰ê· _ì„±ì¥ë¥ '] = daily['revenue_dod'].mean()
                summary['ì„±ì¥ë¥ _í‘œì¤€í¸ì°¨'] = daily['revenue_dod'].std()
            
            if 'trend_direction_7' in daily.columns:
                trend_counts = daily['trend_direction_7'].value_counts().to_dict()
                summary['ìƒìŠ¹_ì¼ìˆ˜'] = trend_counts.get('up', 0)
                summary['í•˜ë½_ì¼ìˆ˜'] = trend_counts.get('down', 0)
                summary['ë³´í•©_ì¼ìˆ˜'] = trend_counts.get('stable', 0)
            
            if 'is_anomaly' in daily.columns:
                summary['ì´ìƒì¹˜_ê±´ìˆ˜'] = daily['is_anomaly'].sum()
                summary['ì´ìƒì¹˜_ë¹„ìœ¨'] = daily['is_anomaly'].mean() * 100
            
            if 'cv_30' in daily.columns:
                summary['í‰ê· _ë³€ë™ì„±'] = daily['cv_30'].mean()
        
        return summary
    
    def clear_cache(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        cache_files = list(self.cache_dir.glob("*.pkl"))
        for file in cache_files:
            file.unlink()
        print(f"ğŸ—‘ï¸ {len(cache_files)}ê°œ ìºì‹œ íŒŒì¼ ì‚­ì œ")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """ìºì‹œ ì •ë³´ ë°˜í™˜"""
        cache_files = list(self.cache_dir.glob("*.pkl"))
        total_size = sum(f.stat().st_size for f in cache_files) / (1024 * 1024)  # MB
        
        return {
            'cache_dir': str(self.cache_dir),
            'file_count': len(cache_files),
            'total_size_mb': round(total_size, 2),
            'ttl_seconds': self.cache_ttl
        }


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_trend_tables(db_path="schedule.db"):
    """
    ì¶”ì„¸ë¶„ì„ìš© í…Œì´ë¸” ìƒì„±
    
    Parameters:
    -----------
    db_path : str
        ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    print("ğŸ”¨ ì¶”ì„¸ë¶„ì„ í…Œì´ë¸” ìƒì„± ì¤‘...")
    
    # trend_daily í…Œì´ë¸”
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS trend_daily (
        date TEXT PRIMARY KEY,
        revenue REAL,
        units_sold INTEGER,
        broadcast_count INTEGER,
        avg_roi REAL,
        revenue_dod REAL,
        revenue_wow REAL,
        revenue_mom REAL,
        revenue_yoy REAL,
        ma_7 REAL,
        ma_30 REAL,
        ma_90 REAL,
        trend_direction TEXT,
        volatility REAL,
        z_score REAL,
        is_anomaly INTEGER,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)
    
    # trend_weekly í…Œì´ë¸”
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS trend_weekly (
        year_week TEXT PRIMARY KEY,
        start_date TEXT,
        end_date TEXT,
        revenue REAL,
        units_sold INTEGER,
        broadcast_count INTEGER,
        avg_roi REAL,
        revenue_wow REAL,
        revenue_4w REAL,
        trend_score REAL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)
    
    # trend_monthly í…Œì´ë¸”
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS trend_monthly (
        year_month TEXT PRIMARY KEY,
        revenue REAL,
        units_sold INTEGER,
        broadcast_count INTEGER,
        avg_roi REAL,
        revenue_mom REAL,
        revenue_yoy REAL,
        seasonal_index REAL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)
    
    # trend_category í…Œì´ë¸”
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS trend_category (
        date TEXT,
        category TEXT,
        revenue REAL,
        market_share REAL,
        growth_rate REAL,
        trend_direction TEXT,
        momentum_score REAL,
        PRIMARY KEY (date, category)
    )
    """)
    
    # ì¸ë±ìŠ¤ ìƒì„±
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_trend_daily_date ON trend_daily(date)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_trend_weekly_week ON trend_weekly(year_week)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_trend_monthly_month ON trend_monthly(year_month)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_trend_category_date ON trend_category(date)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_trend_category_cat ON trend_category(category)")
    
    conn.commit()
    conn.close()
    
    print("âœ… ì¶”ì„¸ë¶„ì„ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")


def populate_trend_tables(db_path="schedule.db"):
    """
    ê¸°ì¡´ ë°ì´í„°ë¡œ ì¶”ì„¸ í…Œì´ë¸” ì±„ìš°ê¸°
    
    Parameters:
    -----------
    db_path : str
        ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
    """
    pipeline = TrendDataPipeline(db_path)
    
    print("ğŸ“Š ì¶”ì„¸ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ì „ì²´ ë°ì´í„°ë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    result = pipeline.execute_pipeline(use_cache=False, source='schedule')
    
    if not result or 'daily' not in result:
        print("âŒ ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
        return
    
    conn = sqlite3.connect(db_path)
    
    try:
        # ì¼ë³„ ë°ì´í„° ì €ì¥
        if 'daily' in result and not result['daily'].empty:
            daily = result['daily'].copy()
            daily['date'] = daily['date'].dt.strftime('%Y-%m-%d')
            daily.to_sql('trend_daily', conn, if_exists='replace', index=False)
            print(f"âœ” ì¼ë³„ ì¶”ì„¸: {len(daily)}ê°œ ë ˆì½”ë“œ")
        
        # ì£¼ë³„ ë°ì´í„° ì €ì¥
        if 'weekly' in result and not result['weekly'].empty:
            weekly = result['weekly'].copy()
            if 'start_date' in weekly.columns:
                weekly['start_date'] = pd.to_datetime(weekly['start_date']).dt.strftime('%Y-%m-%d')
            if 'end_date' in weekly.columns:
                weekly['end_date'] = pd.to_datetime(weekly['end_date']).dt.strftime('%Y-%m-%d')
            weekly.to_sql('trend_weekly', conn, if_exists='replace', index=False)
            print(f"âœ” ì£¼ë³„ ì¶”ì„¸: {len(weekly)}ê°œ ë ˆì½”ë“œ")
        
        # ì›”ë³„ ë°ì´í„° ì €ì¥
        if 'monthly' in result and not result['monthly'].empty:
            monthly = result['monthly']
            monthly.to_sql('trend_monthly', conn, if_exists='replace', index=False)
            print(f"âœ” ì›”ë³„ ì¶”ì„¸: {len(monthly)}ê°œ ë ˆì½”ë“œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ì €ì¥
        if 'category' in result and not result['category'].empty:
            category = result['category'].copy()
            if 'date' in category.columns:
                category['date'] = pd.to_datetime(category['date']).dt.strftime('%Y-%m-%d')
            # week ì»¬ëŸ¼ì€ ì œì™¸í•˜ê³  ì €ì¥
            columns_to_save = ['date', 'category', 'revenue', 'market_share', 
                              'growth_rate', 'trend_direction', 'momentum_score']
            columns_to_save = [col for col in columns_to_save if col in category.columns]
            
            if 'week' in category.columns and 'date' not in category.columns:
                # weekë¥¼ dateë¡œ ë³€í™˜
                category['date'] = pd.to_datetime(category['week'] + '-1', format='%Y-W%U-%w')
                category['date'] = category['date'].dt.strftime('%Y-%m-%d')
            
            if columns_to_save:
                category[columns_to_save].to_sql('trend_category', conn, 
                                                if_exists='replace', index=False)
                print(f"âœ” ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì„¸: {len(category)}ê°œ ë ˆì½”ë“œ")
        
        conn.commit()
        print("\nâœ… ì¶”ì„¸ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        conn.rollback()
    finally:
        conn.close()


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸ“Š ì¶”ì„¸ë¶„ì„ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…Œì´ë¸” ìƒì„±
    create_trend_tables()
    
    # íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    pipeline = TrendDataPipeline()
    
    # ìµœê·¼ 30ì¼ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    from datetime import datetime, timedelta
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
    
    result = pipeline.execute_pipeline(
        date_range=(start_date, end_date),
        filters={'platforms': ['NSí™ˆì‡¼í•‘']},
        use_cache=True,
        source='schedule'  # schedule í…Œì´ë¸” ì‚¬ìš©
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“ˆ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼:")
    for key, df in result.items():
        if isinstance(df, pd.DataFrame):
            print(f"  - {key}: {len(df)}ê°œ ë ˆì½”ë“œ")
        elif isinstance(df, dict):
            print(f"  - {key}: {len(df)}ê°œ í•­ëª©")
    
    # ìºì‹œ ì •ë³´
    cache_info = pipeline.get_cache_info()
    print(f"\nğŸ’¾ ìºì‹œ ì •ë³´:")
    print(f"  - íŒŒì¼ ìˆ˜: {cache_info['file_count']}ê°œ")
    print(f"  - ì´ í¬ê¸°: {cache_info['total_size_mb']}MB")
    print(f"  - TTL: {cache_info['ttl_seconds']}ì´ˆ")
    
    print("\nâœ¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")