name: Daily Labangba Scraping

on:
  schedule:
    # KST ê¸°ì¤€ ì‹¤í–‰ ì‹œê°„ (UTC = KST - 9ì‹œê°„)
    # ì˜¤ì „/ì˜¤í›„ 2ì‹œê°„ ê°„ê²©
    - cron: '0 22 * * *'   # 07:00 KST
    - cron: '0 0 * * *'    # 09:00 KST
    - cron: '0 2 * * *'    # 11:00 KST  
    - cron: '0 4 * * *'    # 13:00 KST
    - cron: '0 6 * * *'    # 15:00 KST
    - cron: '0 8 * * *'    # 17:00 KST
    - cron: '0 10 * * *'   # 19:00 KST
    - cron: '0 12 * * *'   # 21:00 KST
    # 23ì‹œ ì´í›„ ì§‘ì¤‘ í¬ë¡¤ë§
    - cron: '0 14 * * *'   # 23:00 KST
    - cron: '30 14 * * *'  # 23:30 KST
    - cron: '45 14 * * *'  # 23:45 KST
    - cron: '56 14 * * *'  # 23:56 KST
    
  workflow_dispatch:
    inputs:
      force_run:
        description: 'ìˆ˜ë™ ì‹¤í–‰'
        required: false
        default: 'false'

permissions:
  contents: write
  actions: read

jobs:
  scrape-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
    
    - name: Install dependencies
      run: |
        pip install requests pandas openpyxl streamlit plotly
        
    - name: Run health check
      continue-on-error: true
      env:
        LABANGBA_COOKIE: ${{ secrets.LABANGBA_COOKIE }}
      run: |
        if [ -f "health_check.py" ]; then
          python health_check.py || echo "Health check warnings"
        fi
    
    - name: Run scraping
      env:
        LABANGBA_COOKIE: ${{ secrets.LABANGBA_COOKIE }}
      run: |
        echo "í¬ë¡¤ë§ ì‹œìž‘: $(date +%Y-%m-%d' '%H:%M:%S)"
        
        # Cookie ì—…ë°ì´íŠ¸
        if [ -f "cookie_updater.py" ]; then
          python cookie_updater.py
        fi
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        if [ -f "scrape_schedule.py" ]; then
          python scrape_schedule.py
        else
          echo "ERROR: scrape_schedule.py not found"
          exit 1
        fi
    
    - name: Check data quality
      id: data_check
      continue-on-error: true
      run: |
        if [ -f "check_data.py" ]; then
          python check_data.py
        fi
    
    - name: Update aggregate tables
      if: success()
      continue-on-error: true
      run: |
        if [ -f "update_aggregate_tables.py" ]; then
          python update_aggregate_tables.py
        fi
    
    - name: Generate README
      if: always()
      run: |
        if [ -f "generate_readme.py" ]; then
          python generate_readme.py
        fi
    
    - name: Commit and push
      if: always()
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        git add -A
        
        # ì»¤ë°‹ ë©”ì‹œì§€ ìƒì„±
        MESSAGE="ðŸ“Š Update: $(date +%Y-%m-%d' '%H:%M' KST')"
        if [ -f "data_check.json" ]; then
          TOTAL=$(python3 -c "import json; print(json.load(open('data_check.json')).get('today', 0))" 2>/dev/null || echo "0")
          SALES=$(python3 -c "import json; print(json.load(open('data_check.json')).get('with_sales', 0))" 2>/dev/null || echo "0")
          MESSAGE="ðŸ“Š Update: $(date +%Y-%m-%d' '%H:%M) - $TOTAL records, $SALES with sales"
        fi
        
        git diff --quiet && git diff --staged --quiet || {
          git commit -m "$MESSAGE"
          git push origin main
        }
